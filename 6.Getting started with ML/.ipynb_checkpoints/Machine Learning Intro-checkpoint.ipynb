{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- creates a statistical model to mimic intelligent decision making\n",
    "- finding patterns in complex, scattered data to present information.\n",
    "- building smart application/devices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Machine learning is subfield of AI concerned with algorithms that allow computers to learn.\n",
    "- it means that in most cases an algorithm is given a set of data and infers information about properties of data\n",
    "- this information allows it to make predictions about other data that it might see in future "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"1/hierarchy.png\" width=500 height=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- e.g., of smart applications photo based applications,chatbots, voice recognition, pattern analysis, digital security"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Other examples are Robotics, Game AI, recommendation system, psuedo-creative AI(example is Prisma)\n",
    "- In prisma we trained model to paint face like an artist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Others are also using it\n",
    "- Google Page ranking\n",
    "- Netflix suggestions\n",
    "- tinder for us to \"chill\"\n",
    "- Uber tesla self driving cars running in California\n",
    "- Political Campaign and advertisment campaign\n",
    "- spam filtering\n",
    "- google adsense\n",
    "- bio-informatics\n",
    "- google \"Allo\", amazon \"Alexa\"\n",
    "- Facebook photo tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few important features\n",
    "- Input features x=(x1,x2,x3,x4....)\n",
    "- Training set : has list of input features and output too ([x1,y1\n",
    "                                                             x2,y2\n",
    "                                                             x3,y3\n",
    "                                                             .,.\n",
    "                                                             .,.\n",
    "                                                             .,.])\n",
    "- Test set contains a new data apart from training set ([x1,\n",
    "                                                        x2,\n",
    "                                                        x3,\n",
    "                                                        .\n",
    "                                                        .\n",
    "                                                        .).\n",
    "  Once machine has completed its learning on training set it will predict output for test set\n",
    "- Hypothesis will map input(X) to output(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $x^{(i)}$   : ith example of x\n",
    "- $x_{j}^{(i)}$ : jth feature of ith example of x\n",
    "- since x ={$x_{1}$,$x_{2}$,.....}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- assume $x_{1}$ is area and $x_{2}$ is floor .\n",
    "- Then x=[Area, floor]={$x_{1}$,$x_{2}$} which is $x^{(i)}$ here\n",
    "- and area or floor is one of the features from this example set\n",
    "- $y^{(i)}$ represents output for this ith example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SuperVised Learning \n",
    "\n",
    "- means we try to predict output for test set because we have x and y both in training set .ie., we have label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two types\n",
    "- Regression : in which we predict continuous valued outputs. For e.g., housing price, stock price based on certain features. Continuous valued outputs means y=f(x). x here, is input feature set.\n",
    "- Classification : in this we have discrete classes. For e.g., we have apples, guava, mango etc. When new fruit is given then we will try to map it to given set of classes of fruits i.e, gauva, mango, apple etc.\n",
    "  These classes ahve certain features like shape etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "- assume we have been given training set with labeled data ie,, area and price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>400</td>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>600</td>\n",
       "      <td>1150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>2100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Area  Price\n",
       "0   400    800\n",
       "1   600   1150\n",
       "2  1000   2100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "    \n",
    "import pandas as pd\n",
    "data = [[400,800], [600,1150], [1000,2100]]\n",
    "pd.DataFrame(data, columns=[\"Area\", \"Price\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now if new area is given assume 700 what we can predict about its price?\n",
    "- we need y=f(x) from the above given training set\n",
    "- it can be predicted around 1300.\n",
    "- i.e., if we will plot the graph for the above table we will get line and the price for 700 will be around this line only.\n",
    "- we can write line as y =mx+c or y=$\\theta_{1}$x+$\\theta_{0}$\n",
    "- hypothesis helps us in finding this $\\theta_{1}$ and $\\theta_{0}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "h(x)=\\sum_{i=0}^n \\theta_i x_i,    x_{0}=1  \n",
    "\\end{equation*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have so many lines plotted for training set then which line is best?\n",
    "- the one which has minimum error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"1/error.png\" width=500 height=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to calculate this error?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "    total error=\\sum_{i=1}^m |y^{(i)}-h_\\theta(x^{(i)})|\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where m is no of training examples\n",
    "- but this mod function is not differentiable\n",
    "- we need to make few changes\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "    total squared error=\\sum_{i=1}^m (y^{(i)}-h_\\theta(x^{(i)}))^2\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- this is differentiable\n",
    "- it has nice probabilistic interpretition(discussed later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most of the machine learning algorithms is about optimizing i.e minimizing errors over all training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- here error function is polynomial of degree 2\n",
    "- error is function of $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we know that error depends on line in case of linear regression\n",
    "- the more close line is we have less errors for our training examples\n",
    "- therefore for quadratic error we can say that we need minimum error that is minima.\n",
    "- our error is convex function of $\\theta$\n",
    "\n",
    "What is convex function?\n",
    "- whenever we draw line b/w any two points in graph the function should go only below the line.\n",
    "- it has local minima similar to global minima\n",
    "- for local minima we need "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "|\\frac{\\partial f(\\theta)}{\\partial \\theta}|=0\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"1/gradientdescenterror.png\" width=500 height=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\theta=\\theta- \\alpha\\frac{\\partial f(\\theta)}{\\partial \\theta}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- here $\\alpha$ is learning rate which should be small. Why?\n",
    "- Because learning rate is nothing but steps we have to take to reach local minima from certain point i.e., for given $\\theta$\n",
    "- This update rule algorithm will stop when derivative is zero means once we reach local minima then \n",
    "- $\\theta$=$\\theta$-$\\alpha$*0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"1/update.png\" width=500 height=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also this total square error can be modified as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "    total sqaured error=\\sum_{i=1}^m (y^{(i)}-h_\\theta(x^{(i)}))^2\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "    f(\\theta_{0},\\theta_{1})(x_{i})=\\sum_{i=1}^m (y^{(i)}-(\\theta_{0}+\\theta_{1}x))^2\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "    \\theta_{0}=\\theta_{0}-\\alpha\\frac{\\partial f(\\theta_{0})}{\\partial \\theta_{0}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "    \\theta_{1}=\\theta_{1}-\\alpha\\frac{\\partial f(\\theta_{1})}{\\partial \\theta_{1}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"1/a.png\" width=500 height=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"1/1.png\" width=700 height=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"1/2.png\" width=700 height=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"1/3.png\" width=700 height=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fit $\\theta$ to get minimum error i.e., find $\\theta_{0}$  and $\\theta_{1}$ \n",
    "- After that to predict the output use formula   $\\theta^{T}x$ \n",
    "- where x is a point is from test set and $\\theta$ is an array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Linear Regression algo\n",
    "\n",
    "- matplotlib is used for plotting graph\n",
    "- numpy for maths\n",
    "- pandas for reading ad writing to csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (X) Milk acidity , (Y) Density of milk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# values will convert it into arrays\n",
    "# shape will print array size\n",
    "def readData(filename):\n",
    "    df=pd.read_csv(filename)\n",
    "    return df.values\n",
    "\n",
    "x=readData('./X.csv')\n",
    "y=readData('./Y.csv')\n",
    "print(x)\n",
    "print(x.shape)\n",
    "\n",
    "#reshape converts array of array to 1-D array \n",
    "print(x.reshape((99,))\n",
    "print(y.reshape((99,))\n",
    "      \n",
    "#plot function joins points and forms a line      \n",
    "plt.plot(x,y)\n",
    "      \n",
    "#instead we can use scatter plot to see only points\n",
    "plt.scatter(x,y)\n",
    "plt.show()\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization step(optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generally we make mean=0.Why?\n",
    "So that all the features which are far from origin should come close to origin \n",
    "- and standard deviation=1. Why?\n",
    "So that all the features are scaled from 0 to 1 range\n",
    "- Doing normalization doesn't changes the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=x-x.mean()/(x.std())\n",
    "plt.scatter(x,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm- Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypothesis(theta,x):\n",
    "    return theta[0]+theta[1]*x\n",
    "\n",
    "def error(X,Y,theta):\n",
    "    total_error=0\n",
    "    \n",
    "    //to get number of training examples\n",
    "    m=X.shape[0]\n",
    "    \n",
    "    for i in range(m):\n",
    "        total_error+= (Y[i]-hypothesis(theta,X[i]))**2\n",
    "    \n",
    "    return 0.5*total_error \n",
    "\n",
    "def gradient(X,Y,theta):\n",
    "    grad=np.array([0.0,0.0])\n",
    "    m=X.shape[0]\n",
    "    \n",
    "    #applying the formula of J($\\theta$)\n",
    "    for i in range(m):\n",
    "        grad[0]+=-1*(Y[i]-hypothesis(theta,X[i]))\n",
    "        grad[1]+=-1*(Y[i]-hypothesis(theta,X[i]))*X[i]\n",
    "    return grad\n",
    "    \n",
    "\n",
    "def gradient_descent(X,Y,learning_rate,maxitr):\n",
    "    \n",
    "    # since we have derivative of theta 0/theta 0  and derivative of theta 1/theta 1\n",
    "    # so we need grad of size 1*2\n",
    "    # similarly for theta we have theta 0 and theta 1\n",
    "    \n",
    "    grad=np.array([0.0,0.0])\n",
    "    theta=np.array([0.0,0.0])\n",
    "    \n",
    "    #error is list\n",
    "    e=[]\n",
    "    \n",
    "    for i in range(maxitr):\n",
    "        grad=gradient(X,Y,theta)\n",
    "        ce=error(X,Y,theta)\n",
    "        theta[0]=theta[0]-learning_rate*grad[0]\n",
    "        theta[1]=theta[1]-learning_rate*grad[1]\n",
    "        e.append(ce)\n",
    "    \n",
    "    return theta,e\n",
    "    \n",
    "    \n",
    "# maxitr is max iterations for now is taken as 100 \n",
    "# from error graph we can see when the graph has become constant which is after 40 \n",
    "#learning rate wa assumed to be very low\n",
    "\n",
    "theta,e = gradient_descent(X,Y,learning_rate=0.001,maxitr=100)\n",
    "print(theta[0],theta[1])\n",
    "\n",
    "plt.scatter(X,Y)\n",
    "plt.plot(X,hypothesis(theta,X),color='r')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(e)\n",
    "\n",
    "# we can see that the value of both after 50 iterations and 99 iterations is almost same\n",
    "# so maxitr can be 50 or 99\n",
    "print(e[50])\n",
    "print(e[99])\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Note:  Linear Regression is classification and Supervised Learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
