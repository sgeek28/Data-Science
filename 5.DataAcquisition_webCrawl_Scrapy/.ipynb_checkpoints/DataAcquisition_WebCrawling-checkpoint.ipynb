{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An open source and collaborative framework for extracting the data that we need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scrapy in /home/sneha/.local/lib/python3.7/site-packages (1.7.3)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in /home/sneha/.local/lib/python3.7/site-packages (from scrapy) (1.21.0)\n",
      "Requirement already satisfied: service-identity in /home/sneha/.local/lib/python3.7/site-packages (from scrapy) (18.1.0)\n",
      "Requirement already satisfied: lxml; python_version != \"3.4\" in /usr/lib/python3/dist-packages (from scrapy) (4.3.2)\n",
      "Requirement already satisfied: cssselect>=0.9 in /home/sneha/.local/lib/python3.7/site-packages (from scrapy) (1.1.0)\n",
      "Requirement already satisfied: pyOpenSSL in /usr/lib/python3/dist-packages (from scrapy) (19.0.0)\n",
      "Requirement already satisfied: Twisted>=13.1.0; python_version != \"3.4\" in /home/sneha/.local/lib/python3.7/site-packages (from scrapy) (19.7.0)\n",
      "Requirement already satisfied: queuelib in /home/sneha/.local/lib/python3.7/site-packages (from scrapy) (1.5.0)\n",
      "Requirement already satisfied: parsel>=1.5 in /home/sneha/.local/lib/python3.7/site-packages (from scrapy) (1.5.2)\n",
      "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from scrapy) (1.11.0)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in /home/sneha/.local/lib/python3.7/site-packages (from scrapy) (2.0.5)\n",
      "Requirement already satisfied: cryptography in /usr/lib/python3/dist-packages (from service-identity->scrapy) (2.6.1)\n",
      "Requirement already satisfied: attrs>=16.0.0 in /home/sneha/.local/lib/python3.7/site-packages (from service-identity->scrapy) (19.1.0)\n",
      "Requirement already satisfied: pyasn1 in /usr/lib/python3/dist-packages (from service-identity->scrapy) (0.4.2)\n",
      "Requirement already satisfied: pyasn1-modules in /home/sneha/.local/lib/python3.7/site-packages (from service-identity->scrapy) (0.2.6)\n",
      "Requirement already satisfied: PyHamcrest>=1.9.0 in /home/sneha/.local/lib/python3.7/site-packages (from Twisted>=13.1.0; python_version != \"3.4\"->scrapy) (1.9.0)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in /home/sneha/.local/lib/python3.7/site-packages (from Twisted>=13.1.0; python_version != \"3.4\"->scrapy) (19.0.0)\n",
      "Requirement already satisfied: incremental>=16.10.1 in /home/sneha/.local/lib/python3.7/site-packages (from Twisted>=13.1.0; python_version != \"3.4\"->scrapy) (17.5.0)\n",
      "Requirement already satisfied: constantly>=15.1 in /home/sneha/.local/lib/python3.7/site-packages (from Twisted>=13.1.0; python_version != \"3.4\"->scrapy) (15.1.0)\n",
      "Requirement already satisfied: zope.interface>=4.4.2 in /home/sneha/.local/lib/python3.7/site-packages (from Twisted>=13.1.0; python_version != \"3.4\"->scrapy) (4.6.0)\n",
      "Requirement already satisfied: Automat>=0.3.0 in /home/sneha/.local/lib/python3.7/site-packages (from Twisted>=13.1.0; python_version != \"3.4\"->scrapy) (0.7.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from PyHamcrest>=1.9.0->Twisted>=13.1.0; python_version != \"3.4\"->scrapy) (40.8.0)\n",
      "Requirement already satisfied: idna>=2.5 in /usr/lib/python3/dist-packages (from hyperlink>=17.1.1->Twisted>=13.1.0; python_version != \"3.4\"->scrapy) (2.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: scrapy.cfg already exists in /home/sneha/Downloads/DataScience-CodingBlocks/DataAcquisition_webCrawl_Scrapy/myproject\r\n"
     ]
    }
   ],
   "source": [
    "# to create a project folder\n",
    "# !scrapy startproject projectname\n",
    "# go back and check the directory\n",
    "!scrapy startproject myproject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Spider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- follow this url for learning more about spider\n",
    "- http://docs.scrapy.org/en/latest/intro/tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Go to myproject folder \n",
    "- Go to spider folder\n",
    "- Create new quotes_spider.py file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After writing code go to location of .py file\n",
    "- Go to Downloads/DataScience-CodingBlocks/DataAcquisition_webCrawl_Scrapy/myproject/myproject\n",
    "- Run scrapy crawl quotes\n",
    "- we will see two html files created quote1.html and quote2.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract data from  a given Web Page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have already done scraping of data using beautiful soup library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- this time we use scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- go to /Downloads/DataScience-CodingBlocks/DataAcquisition_webCrawl_Scrapy/myproject\n",
    "- run this command (scrapy shell 'http://quotes.toscrape.com/page/1/') on shell it will send url request and will return response\n",
    "- we can check this by typing response , in output we can see <200 http://quotes.toscrape.com/page/1/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now we can use this response to fetch the title of url as mentioned above\n",
    "- response.css('title') run this on shell , here title is tag\n",
    "- it will return metadta\n",
    "- we need title only so, response.css('title').getall()\n",
    "- this will return a list\n",
    "- we need first element response.css('title::text').getall()[0]\n",
    "- or we can use get function instead of getAll that by default returns first element\n",
    "- response.css('title::text').get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we can fetch authors and tags for a particular quote\n",
    "- quote=response.css('div.quotes').get()\n",
    "- title=quote.css('span.text::text').get()\n",
    "- author=quote.css('small.author::text').get()\n",
    "- tag=quote.css('a.tag::text').get()\n",
    "- ::text is used to print the content in textual format\n",
    "- span.text means span is tag of htmland text is class name one can check doing inspect element over any quote\n",
    "- for q in response.css('div.quotes'):\n",
    "-     text=q.css('span.text::text').get()\n",
    "-     author=q.css('small.author::text').get()\n",
    "-     tag=q.css('a.tag::text').getall()\n",
    "- we can store this is in dictionary \n",
    "-    yield {\n",
    "     \"text\"=text,\n",
    "     \"author\"=author,\n",
    "     \"tag\"=tag\n",
    "}\n",
    "\n",
    "- we can add all this code in quote.py file of spider folder\n",
    "- we can run this using this command scrapy crawl quotes -o quotes.json\n",
    "- we can even run all this command on shell also using scrapy shell 'http://quotes.toscrape.com/page/1/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "\n",
    "    def start_requests(self):\n",
    "        urls = [\n",
    "            'http://quotes.toscrape.com/page/1/',\n",
    "            'http://quotes.toscrape.com/page/2/',\n",
    "        ]\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        page = response.url.split(\"/\")[-2]\n",
    "        filename = 'quotes-%s.html' % page\n",
    "        \n",
    "        #with open(filename, 'wb') as f:\n",
    "        #    f.write(response.body)\n",
    "        #self.log('Saved file %s' % filename)\n",
    "\n",
    "        for q in response.css(\"div.quote\"):\n",
    "            text=q.css('span.text::text').get()\n",
    "            author=q.css('small.author::text').get()\n",
    "            tags=q.css('a.tag::text').getall()\n",
    "            yield {\n",
    "                \"text\":text,\n",
    "                \"author\":author,\n",
    "                \"tags\":tags\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after this go to folder myprojects and check json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we want to crawl all the quotes from all the pages not only \n",
    "# 1st and last page, then we need to find class of next button on 1st page use following codeas shown below\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "\n",
    "    def start_requests(self):\n",
    "        urls = [\n",
    "            'http://quotes.toscrape.com/page/1/',\n",
    "            'http://quotes.toscrape.com/page/2/',\n",
    "        ]\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        page = response.url.split(\"/\")[-2]\n",
    "        filename = 'quotes-%s.html' % page\n",
    "        \n",
    "        #with open(filename, 'wb') as f:\n",
    "        #    f.write(response.body)\n",
    "        #self.log('Saved file %s' % filename)\n",
    "\n",
    "        for q in response.css(\"div.quote\"):\n",
    "            text=q.css('span.text::text').get()\n",
    "            author=q.css('small.author::text').get()\n",
    "            tags=q.css('a.tag::text').getall()\n",
    "            yield {\n",
    "                \"text\":text,\n",
    "                \"author\":author,\n",
    "                \"tags\":tags\n",
    "            }\n",
    "        \n",
    "        \n",
    "        #new code added\n",
    "        #li.next a::attr(href) means give me anchor tag of li.next and we want to access\n",
    "        #next page url which is present in href , attribute of a(anchor) tag\n",
    "        # we can access next page url in other way also\n",
    "        #response.css('li.next a').attrib[\"href\"]\n",
    "        next_page=response.css('li.next a::attr(href)').get()\n",
    "        if next_page is not None:\n",
    "            \n",
    "            #this will join new link to next_page\n",
    "            next_page=response.urljoin(next_page)\n",
    "            \n",
    "            # will make request will new url next_page and will call parse method until last page\n",
    "            yield scrapy.Request(next_page,callback=self.parse)\n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this command in second myproject folder\n",
    "#scrapy crawl quotes_v2 -o all_quotes.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bookstore scrapy challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class ShopsiteSpider(scrapy.Spider):\n",
    "    name=\"bookstore\"\n",
    "    book_urls=[\n",
    "        'http://books.toscrape.com/catalogue/'\n",
    "    ]\n",
    "    start_urls=[\n",
    "        'http://books.toscrape.com/'\n",
    "    ]\n",
    "    def start_requests(self):\n",
    "        \n",
    "        urls=[\n",
    "            'http://books.toscrape.com/catalogue/page-1.html'\n",
    "        ]\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url,callback=self.parse)\n",
    "        \n",
    "    def parse(self,response):\n",
    "        page_id=response.url.split(\"/\")[4][-6]\n",
    "        filename='books-%s.html'%page_id\n",
    "        \n",
    "        for b in response.css(\"article.product_pod\"):\n",
    "            text=b.css('h3 a::attr(title)').get()\n",
    "            prod_price=b.css('p.price_color::text').get()\n",
    "            book_url=self.book_urls[0]+b.css(\"div a::attr(href)\").get()\n",
    "            img_url=self.start_urls[0]+b.css(\"a img::attr(src)\").get()[3:]\n",
    "            yield{\n",
    "                \"text\":text,\n",
    "                \"prod_price\":prod_price,\n",
    "                \"book_url\":book_url,\n",
    "                \"img_url\":img_url\n",
    "            }\n",
    "        next_page=response.css('ul.pager li.next a::attr(href)').get()\n",
    "        if next_page is not None:\n",
    "            #this will join new link to next_page\n",
    "            next_page=response.urljoin(next_page)\n",
    "            # will make request will new url next_page and will call parse method until last page\n",
    "            yield scrapy.Request(next_page,callback=self.parse)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "page_id=\"http://books.toscrape.com/catalogue/page-1.html\".split(\"/\")\n",
    "print(page_id[4][-6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import scrapy\n",
    "\n",
    "class ShopsiteSpider(scrapy.Spider):\n",
    "    name=\"bookstore\"\n",
    "    book_urls=[\n",
    "        'http://books.toscrape.com/catalogue/'\n",
    "    ]\n",
    "    start_urls=[\n",
    "        'http://books.toscrape.com/'\n",
    "    ]\n",
    "    def start_requests(self):\n",
    "        \n",
    "        urls=[\n",
    "            'http://books.toscrape.com/catalogue/page-1.html'\n",
    "        ]\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url,callback=self.parse)\n",
    "        \n",
    "    def convertToJson(self):\n",
    "        fileInput = 'book.json'\n",
    "        fileOutput = 'book.csv'\n",
    "        inputFile = open(fileInput) #open json file\n",
    "        outputFile = open(fileOutput, 'w') #load csv file\n",
    "        data = json.load(inputFile) #load json content\n",
    "        inputFile.close() #close the input file\n",
    "        output = csv.writer(outputFile) #create a csv.write\n",
    "        output.writerow(data[0].keys())  # header row\n",
    "        for row in data:\n",
    "            output.writerow(row.values()) #values row \n",
    "  \n",
    "   \n",
    "    def parse(self,response):\n",
    "        page_id=response.url.split(\"/\")[4][-6]\n",
    "        filename='books-%s.html'%page_id\n",
    "        \n",
    "        for b in response.css(\"article.product_pod\"):\n",
    "            image_url=b.css(\"a img::attr(src)\").get()\n",
    "            book_title=b.css('h3 a::attr(title)').get()\n",
    "            product_price=b.css('p.price_color::text').get()\n",
    "            #book_url=self.book_urls[0]+b.css(\"div a::attr(href)\").get()\n",
    "            \n",
    "            yield{\n",
    "                \"image_url\":image_url,                \n",
    "                \"book_title\":book_title,\n",
    "                \"product_price\":product_price,\n",
    "                #\"book_url\":book_url,\n",
    "                \n",
    "            }\n",
    "        next_page=response.css('ul.pager li.next a::attr(href)').get()\n",
    "        if next_page is not None:\n",
    "            #this will join new link to next_page\n",
    "            next_page=response.urljoin(next_page)\n",
    "            # will make request will new url next_page and will call parse method until last page\n",
    "            yield scrapy.Request(next_page,callback=self.parse)\n",
    "        \n",
    "        self.convertToJson()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "with open('book.json') as json_file:\n",
    "    data=json.load(json_file)\n",
    "\n",
    "book_data=data['book_details']\n",
    "\n",
    "data_file=open('book_file.csv','w')\n",
    "\n",
    "csv_writer=csv.writer(data_file)\n",
    "\n",
    "count=0\n",
    "\n",
    "for b in book_data:\n",
    "    if count == 0:\n",
    "        header=b.keys()\n",
    "        csv_writer.writerow(header)\n",
    "        count+=1\n",
    "    csv_writer.writerow(b.values())\n",
    "\n",
    "data_file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class ShopsiteSpider(scrapy.Spider):\n",
    "    name=\"bookstore\"\n",
    "    book_urls=[\n",
    "        'http://books.toscrape.com/catalogue/'\n",
    "    ]\n",
    "    start_urls=[\n",
    "        'http://books.toscrape.com/'\n",
    "    ]\n",
    "    def start_requests(self):\n",
    "        \n",
    "        urls=[\n",
    "            'http://books.toscrape.com/catalogue/page-1.html'\n",
    "        ]\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url,callback=self.parse)\n",
    "   \n",
    "    def parse(self,response):\n",
    "        page_id=response.url.split(\"/\")[4][-6]\n",
    "        filename='books-%s.html'%page_id\n",
    "       \n",
    "        for b in response.css(\"article.product_pod\"):\n",
    "            book_titles=''\n",
    "            image_url=b.css(\"a img::attr(src)\").get()\n",
    "            book_title=b.css('h3 a::attr(title)').get()\n",
    "            product_price=b.css('p.price_color::text').get()\n",
    "            #book_url=self.book_urls[0]+b.css(\"div a::attr(href)\").get()\n",
    "            \n",
    "            if \",\" in book_title:\n",
    "                book_titles+='\"'+book_title+'\"'\n",
    "            else:\n",
    "                book_titles+=book_title\n",
    "\n",
    "            yield{\n",
    "                \"image_url\":image_url,                \n",
    "                \"book_title\":book_titles,\n",
    "                \"product_price\":product_price,\n",
    "                #\"book_url\":book_url,\n",
    "                \n",
    "            }\n",
    "        next_page=response.css('ul.pager li.next a::attr(href)').get()\n",
    "        if next_page is not None:\n",
    "            #this will join new link to next_page\n",
    "            next_page=response.urljoin(next_page)\n",
    "            # will make request will new url next_page and will call parse method until last page\n",
    "            yield scrapy.Request(next_page,callback=self.parse)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "class BookSpider(scrapy.Spider):\n",
    "    name = \"books_spider\"\n",
    "    def start_requests(self):\n",
    "        urls = [\n",
    "            \"http://books.toscrape.com/catalogue/page-1.html\"\n",
    "        ]\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "\n",
    "        for q in response.css(\"article.product_pod\"):\n",
    "            link = q.css(\"div.image_container a img::attr(src)\").get()\n",
    "            title = q.css(\"h3 a::attr(title)\").get()\n",
    "            price = q.css(\"div.product_price p.price_color::text\").get()\n",
    "            \n",
    "            yield {\n",
    "                'image_url' : link,\n",
    "                'book_title' : title,\n",
    "                'product_price': price\n",
    "            }\n",
    "        next_page = response.css('ul.pager li.next a::attr(href)').get()\n",
    "        if next_page is not None:\n",
    "            \n",
    "            next_page = response.urljoin(next_page)\n",
    "            yield scrapy.Request(next_page,callback = self.parse)\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class pepperfry(scrapy.Spider):\n",
    "    name=\"pepperfrySpider\"\n",
    "    BASE_DIR='./Pepperfry_data/'\n",
    "    MAX_CNT=20\n",
    "    \n",
    "    \n",
    "    def start_requests(self):\n",
    "        \n",
    "        BASE_URL=\"https://www.pepperfry.com/site_product/search?q=\"\n",
    "        \n",
    "        \n",
    "        items=[\"two seater sofa\",\"bench\",\"book cases\",\"coffee table\",\n",
    "              \"dining set\",\"queen beds\",\"arm chairs\",\"chest drawers\",\n",
    "              \"garden seating\",\"bean bags\",\"king beds\"]\n",
    "        \n",
    "        urls=[]\n",
    "        dir_names=[]\n",
    "        \n",
    "        for item in items:\n",
    "            query_string='-'.join(item.split(' '))\n",
    "            dir_name=' '.join(item.split(' '))\n",
    "            dir_names.append(dir_name)\n",
    "            urls.append(BASE_URL+query_string)\n",
    "            \n",
    "            dir_path=self.BASE_DIR+dir_name\n",
    "            if not os.path.exists(dir_path):\n",
    "                os.makedirs(dir_path)\n",
    "                \n",
    "        for i in range(len(urls)):\n",
    "            d={\n",
    "                \"dir_name\":dir_names[i]\n",
    "            }\n",
    "            resp=scrapy.Request(url=urls[i],callback=self.parse,\n",
    "                               dont_filter=True)\n",
    "            resp.meta['dir_name']=dir_names[i]\n",
    "            yield resp\n",
    "            \n",
    "            \n",
    "    def parse(self,response,**meta):\n",
    "        #response.selector.xpath('').extract()\n",
    "        product_urls=response.xpath('//div/div/div/a[@p=0]/@href').extract()\n",
    "        #print(product_urls)\n",
    "        #print(len(product_urls))\n",
    "        counter=0\n",
    "        \n",
    "        #print(response.meta)\n",
    "        for url in product_urls:\n",
    "            resp=scrapy.Request(url=url,callback=self.parse_item,dont_filter=True)\n",
    "            resp.meta['dir_name']=response.meta['dir_name']\n",
    "            #print(resp)\n",
    "            \n",
    "            if counter == self.MAX_CNT:\n",
    "                break\n",
    "                \n",
    "            if not resp == None:\n",
    "                counter+=1\n",
    "                #print(resp)\n",
    "                \n",
    "            yield resp\n",
    "            \n",
    "    def parse_item(self,response,**meta):\n",
    "        \n",
    "        item_title=response.xpath('//div/div/div/h1/text()').extract()[0]\n",
    "        item_price=response.xpath('//div/div/div/p/b[@class=\"pf-orange-color\n",
    "                                   pf-large font-20 pf-primary-color\"]/text()').extract()[0].strip()\n",
    "        item_savings=response.xpath('//p[@class=\"pf-margin-0 pf-bold-txt font-13\"]/text()').extract()[0].strip()\n",
    "        item_description=response.xpath('')\n",
    "            \n",
    "    \n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
